# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NjMfodXqlc12mmTmzqGH_gOp_gvfo7VB

Gan 참고
https://m.blog.naver.com/qbxlvnf11/221529286431

텐서플로 공식사이트
https://www.tensorflow.org/tutorials/generative/dcgan?hl=ko

TF.dataset 참고
https://hwiyong.tistory.com/328?category=840057

Gan 참고
https://frhyme.github.io/machine-learning/gan_basic/
"""

import sys
import numpy as np
from keras.layers import Input, Dense, Reshape, Flatten, Dropout, LeakyReLU, BatchNormalization
from keras.models import Sequential, Model
from keras.optimizers import Adam
from keras.datasets import mnist
from random import randint
import tensorflow as tf
import glob
import imageio
import matplotlib.pyplot as plt
import os
import PIL
from tensorflow.keras import layers
import time
from google.colab import files
from tqdm import tqdm

def load_MNIST(model_type) :
  (X_train, y_train), (X_test, y_test) = mnist.load_data()
  X_train = X_train.reshape(-1,28,28,1).astype(np.float32)
  X_train = (X_train - 127.5) / 127.5
  
  if model_type == -1 :
    pass
  else :
    idx = np.where(y_train == model_type)
    X_train = X_train[idx]

  return X_train

cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)
generator_optimizer = tf.keras.optimizers.Adam(1e-4)

def make_discriminator_model():
    model = tf.keras.Sequential()
    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',
                                     input_shape=[28, 28, 1]))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Flatten())
    model.add(layers.Dense(1, activation = 'sigmoid'))

    model.compile(loss = 'binary_crossentropy', optimizer = discriminator_optimizer)

    return model

def make_generator_model():
    model = tf.keras.Sequential()
    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((7, 7, 256)))
    assert model.output_shape == (None, 7, 7, 256) # 주목: 배치사이즈로 None이 주어집니다.

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 7, 7, 128)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 14, 14, 64)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 28, 28, 1)

    model.compile(loss = generator_loss, optimizer = generator_optimizer)

    return model

def create_gan(discriminator, generator):
    discriminator.trainable=False
    gan_input = Input(shape=(noise_dim,))
    x = generator(gan_input)
    gan_output= discriminator(x)
    gan= Model(inputs=gan_input, outputs=gan_output)
    gan.compile(loss='binary_crossentropy', optimizer='adam')
    return gan

def generate_and_save_images(model, epoch, test_input) :
  generated_images =   model(test_input, training = False)
  fig = plt.figure(figsize = (4, 4))
  for i in range(generated_images.shape[0]) :
    plt.subplot(4, 4, i + 1)
    plt.imshow(generated_images[i, :, :, 0] * 127.5 + 127.5, cmap = 'gray')
    plt.axis('off')
  plt.savefig(f'Image_at_epoch_{epoch}.png')
  files.download(f'Image_at_epoch_{epoch}.png')

def train_step(train_dataset) :
  for images in train_dataset :
    real_size = len(images)
    noise = tf.random.normal([real_size, noise_dim])
    
    generated_images = generator.predict(noise)

    X_batch = np.concatenate([images, generated_images])
    y_batch = np.concatenate( [np.ones(real_size), np.zeros(real_size) ] )

    #
    discriminator.trainable = True
    discriminator.train_on_batch(X_batch, y_batch)
    discriminator.trainable = False

    #
    y_gen = np.ones(real_size)

    gan.train_on_batch(noise, y_gen)

X_train = load_MNIST(model_type = -1)
X_train = X_train[:1000]
BUFFER_SIZE = X_train.shape[0]
BATCH_SIZE = 256
HALF_BATCH_SIZE = int( BATCH_SIZE / 2)

train_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(BUFFER_SIZE).batch( HALF_BATCH_SIZE )

##

noise_dim = 100
num_examples_to_generate = 16

test = tf.random.normal([num_examples_to_generate, noise_dim])

##
generator = make_generator_model()
discriminator = make_discriminator_model()
gan = create_gan(discriminator = discriminator,
                generator = generator)

generate_and_save_images(generator, 0, test)

EPOCHS = 500

for e in tqdm(range(EPOCHS)) :
  train_step(train_dataset = train_dataset)

  if (e+1) % 100 == 0 :
    generate_and_save_images(generator, e+1, test)

generator.save('Gan_500_mnist_generator.h5')
discriminator.save('Gan_500_mnist_discriminator.h5')

files.download('Gan_500_mnist_generator.h5')
files.download('Gan_500_mnist_discriminator.h5')