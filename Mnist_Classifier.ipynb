{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mnist Classifier",
      "provenance": [],
      "authorship_tag": "ABX9TyOIzHa0yqgxRZ4rrOgvkhTu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanGyuk-Raccoon/python/blob/master/Mnist_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cd29l0viwrR"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torchvision import transforms, datasets"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O3_W27-i0cJ"
      },
      "source": [
        "lr = 1e-3\n",
        "batch_size = 64\n",
        "num_epoch = 10\n",
        "\n",
        "ckpt_dir = './checkpoint'\n",
        "log_dir = './log'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR1vikv7jbFt"
      },
      "source": [
        "class Net(nn.Module) :\n",
        "  def __init__(self) :\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 10, kernel_size = 5, stride = 1, padding = 0, bias = True)\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.relu1 = nn.ReLU()\n",
        "\n",
        "    self.conv2 = nn.Conv2d(in_channels = 10, out_channels = 20, kernel_size = 5, stride = 1, padding = 0, bias = True)\n",
        "    self.drop2 = nn.Dropout2d(p = 0.5)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
        "    self.relu2 = nn.ReLU()\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features = 320, out_features = 50, bias = True)\n",
        "    self.relu1_fc1 = nn.ReLU()\n",
        "    self.drop1_fc1 = nn.Dropout2d(p = 0.5)\n",
        "\n",
        "    self.fc2 = nn.Linear(in_features = 50, out_features = 10, bias = True)\n",
        "\n",
        "  def forward(self, x) :\n",
        "    x = self.conv1(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.relu1(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.drop2(x)\n",
        "    x = self.pool2(x)\n",
        "    x = self.relu2(x)\n",
        "\n",
        "    x = x.view(-1, 320)\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu1_fc1(x)\n",
        "    x = self.drop1_fc1(x)\n",
        "\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def save(ckpt_dir, net, optim, epoch) :\n",
        "  if not os.path.exists(ckpt_dir) :\n",
        "    os.makedirs(ckpt_dir)\n",
        "\n",
        "  torch.save({'net': net.state_dict(), 'optim' : optim.state_dict()},\n",
        "             f'./{ckpt_dir}/model_epoch{epoch}')\n",
        "  \n",
        "def load(ckpt_dir, net, optim) :\n",
        "  ckpt_lst = os.listdir(ckpt_dir)\n",
        "  ckpt_lst.sort()\n",
        "\n",
        "  dict_model = torch.load(f'./{ckpt_dir}/{ckpt_lst[-1]}')\n",
        "\n",
        "  net.load_state_dict(dict_model['net'])\n",
        "  optim.load_state_dict(dict_model['optim'])\n",
        "\n",
        "  return net, optim"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGQ1COaMl4Ub"
      },
      "source": [
        "transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize(mean = (0.5, ), std = (0.5, ))] )\n",
        "\n",
        "dataset = datasets.MNIST(download = True, root = './', train = True, transform = transform)\n",
        "loader = DataLoader(dataset, batch_size = batch_size, shuffle = True, num_workers = 0)\n",
        "\n",
        "num_data = len(loader.dataset)\n",
        "num_batch = np.ceil(num_data / batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOWz_FyQmZ86",
        "outputId": "cb8da010-bf50-4df7-d6a1-e5348ef4c82e"
      },
      "source": [
        "net = Net().to(device)\n",
        "params = net.parameters()\n",
        "\n",
        "fn_loss = nn.CrossEntropyLoss().to(device)\n",
        "fn_pred = lambda output : torch.softmax(output, dim = 1)\n",
        "fn_acc = lambda pred, label : ( (pred.max(dim = 1)[1] == label).type(torch.float) ).mean()\n",
        "\n",
        "optim = torch.optim.Adam(params, lr = lr)\n",
        "\n",
        "writer = SummaryWriter(log_dir = log_dir)\n",
        "\n",
        "for epoch in range(1, num_epoch + 1) :\n",
        "  net.train()\n",
        "\n",
        "  loss_arr = []\n",
        "  acc_arr = []\n",
        "\n",
        "  for batch, (input, label) in enumerate(loader, 1) :\n",
        "    input = input.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    output = net(input)\n",
        "    pred = fn_pred(output)\n",
        "\n",
        "    optim.zero_grad()\n",
        "\n",
        "    loss = fn_loss(output, label)\n",
        "    acc = fn_acc(pred, label)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optim.step()\n",
        "\n",
        "    loss_arr += [loss.item()]\n",
        "    acc_arr += [acc.item()]\n",
        "\n",
        "    \n",
        "  print(f'TRAIN: {epoch}/{num_epoch} | LOSS: {np.mean(loss_arr) : .4f} | ACC: {np.mean(acc_arr) : .4f}')\n",
        "  writer.add_scalar('loss', np.mean(loss_arr), epoch)\n",
        "  writer.add_scalar('acc', np.mean(acc_arr), epoch)\n",
        "\n",
        "  save(ckpt_dir = ckpt_dir, net = net, optim = optim, epoch = epoch)\n",
        "\n",
        "writer.close()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: 1/10 | LOSS:  0.5832/ | ACC:  0.8136\n",
            "TRAIN: 2/10 | LOSS:  0.2647/ | ACC:  0.9217\n",
            "TRAIN: 3/10 | LOSS:  0.2165/ | ACC:  0.9367\n",
            "TRAIN: 4/10 | LOSS:  0.1919/ | ACC:  0.9429\n",
            "TRAIN: 5/10 | LOSS:  0.1795/ | ACC:  0.9477\n",
            "TRAIN: 6/10 | LOSS:  0.1658/ | ACC:  0.9503\n",
            "TRAIN: 7/10 | LOSS:  0.1615/ | ACC:  0.9527\n",
            "TRAIN: 8/10 | LOSS:  0.1553/ | ACC:  0.9545\n",
            "TRAIN: 9/10 | LOSS:  0.1489/ | ACC:  0.9557\n",
            "TRAIN: 10/10 | LOSS:  0.1429/ | ACC:  0.9575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LiU9vzincla",
        "outputId": "4f4d9aa2-56d3-4392-d499-249c3280e6d7"
      },
      "source": [
        "dataset = datasets.MNIST(download = True, root = './', train = False, transform = transform)\n",
        "loader = DataLoader(dataset, batch_size = batch_size, shuffle = False, num_workers = 0)\n",
        "\n",
        "num_data = len(loader.dataset)\n",
        "num_batch = np.ceil(num_data / batch_size)\n",
        "\n",
        "net, optim = load(ckpt_dir = ckpt_dir, net = net, optim = optim)\n",
        "\n",
        "with torch.no_grad() :\n",
        "  net.eval()\n",
        "\n",
        "  loss_arr = []\n",
        "  acc_arr = []\n",
        "\n",
        "  for batch, (input, label) in enumerate(loader, 1) :\n",
        "    input = input.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    output = net(input)\n",
        "    pred = fn_pred(output)\n",
        "\n",
        "    loss = fn_loss(output, label)\n",
        "    acc = fn_acc(pred, label)\n",
        "\n",
        "    loss_arr += [loss.item()]\n",
        "    acc_arr += [acc.item()]\n",
        "\n",
        "    print(f'TEST| BATCH {batch : 4.0f}/{num_batch : 4.0f} | LOSS: {np.mean(loss_arr) : .4f} | ACC: {np.mean(acc_arr) : .4f}')\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEST| BATCH    1/ 157 | LOSS:  0.0053 | ACC:  1.0000\n",
            "TEST| BATCH    2/ 157 | LOSS:  0.0059 | ACC:  1.0000\n",
            "TEST| BATCH    3/ 157 | LOSS:  0.0103 | ACC:  0.9948\n",
            "TEST| BATCH    4/ 157 | LOSS:  0.0137 | ACC:  0.9922\n",
            "TEST| BATCH    5/ 157 | LOSS:  0.0186 | ACC:  0.9906\n",
            "TEST| BATCH    6/ 157 | LOSS:  0.0287 | ACC:  0.9870\n",
            "TEST| BATCH    7/ 157 | LOSS:  0.0351 | ACC:  0.9866\n",
            "TEST| BATCH    8/ 157 | LOSS:  0.0417 | ACC:  0.9863\n",
            "TEST| BATCH    9/ 157 | LOSS:  0.0383 | ACC:  0.9878\n",
            "TEST| BATCH   10/ 157 | LOSS:  0.0431 | ACC:  0.9859\n",
            "TEST| BATCH   11/ 157 | LOSS:  0.0422 | ACC:  0.9858\n",
            "TEST| BATCH   12/ 157 | LOSS:  0.0449 | ACC:  0.9857\n",
            "TEST| BATCH   13/ 157 | LOSS:  0.0426 | ACC:  0.9856\n",
            "TEST| BATCH   14/ 157 | LOSS:  0.0421 | ACC:  0.9855\n",
            "TEST| BATCH   15/ 157 | LOSS:  0.0455 | ACC:  0.9833\n",
            "TEST| BATCH   16/ 157 | LOSS:  0.0478 | ACC:  0.9824\n",
            "TEST| BATCH   17/ 157 | LOSS:  0.0488 | ACC:  0.9816\n",
            "TEST| BATCH   18/ 157 | LOSS:  0.0479 | ACC:  0.9826\n",
            "TEST| BATCH   19/ 157 | LOSS:  0.0473 | ACC:  0.9827\n",
            "TEST| BATCH   20/ 157 | LOSS:  0.0566 | ACC:  0.9797\n",
            "TEST| BATCH   21/ 157 | LOSS:  0.0598 | ACC:  0.9799\n",
            "TEST| BATCH   22/ 157 | LOSS:  0.0583 | ACC:  0.9801\n",
            "TEST| BATCH   23/ 157 | LOSS:  0.0585 | ACC:  0.9796\n",
            "TEST| BATCH   24/ 157 | LOSS:  0.0616 | ACC:  0.9779\n",
            "TEST| BATCH   25/ 157 | LOSS:  0.0598 | ACC:  0.9788\n",
            "TEST| BATCH   26/ 157 | LOSS:  0.0587 | ACC:  0.9790\n",
            "TEST| BATCH   27/ 157 | LOSS:  0.0608 | ACC:  0.9792\n",
            "TEST| BATCH   28/ 157 | LOSS:  0.0616 | ACC:  0.9782\n",
            "TEST| BATCH   29/ 157 | LOSS:  0.0600 | ACC:  0.9790\n",
            "TEST| BATCH   30/ 157 | LOSS:  0.0606 | ACC:  0.9792\n",
            "TEST| BATCH   31/ 157 | LOSS:  0.0590 | ACC:  0.9798\n",
            "TEST| BATCH   32/ 157 | LOSS:  0.0609 | ACC:  0.9790\n",
            "TEST| BATCH   33/ 157 | LOSS:  0.0617 | ACC:  0.9787\n",
            "TEST| BATCH   34/ 157 | LOSS:  0.0662 | ACC:  0.9779\n",
            "TEST| BATCH   35/ 157 | LOSS:  0.0674 | ACC:  0.9777\n",
            "TEST| BATCH   36/ 157 | LOSS:  0.0678 | ACC:  0.9779\n",
            "TEST| BATCH   37/ 157 | LOSS:  0.0662 | ACC:  0.9785\n",
            "TEST| BATCH   38/ 157 | LOSS:  0.0671 | ACC:  0.9778\n",
            "TEST| BATCH   39/ 157 | LOSS:  0.0671 | ACC:  0.9780\n",
            "TEST| BATCH   40/ 157 | LOSS:  0.0655 | ACC:  0.9785\n",
            "TEST| BATCH   41/ 157 | LOSS:  0.0660 | ACC:  0.9787\n",
            "TEST| BATCH   42/ 157 | LOSS:  0.0662 | ACC:  0.9788\n",
            "TEST| BATCH   43/ 157 | LOSS:  0.0659 | ACC:  0.9786\n",
            "TEST| BATCH   44/ 157 | LOSS:  0.0651 | ACC:  0.9783\n",
            "TEST| BATCH   45/ 157 | LOSS:  0.0640 | ACC:  0.9788\n",
            "TEST| BATCH   46/ 157 | LOSS:  0.0640 | ACC:  0.9783\n",
            "TEST| BATCH   47/ 157 | LOSS:  0.0641 | ACC:  0.9784\n",
            "TEST| BATCH   48/ 157 | LOSS:  0.0636 | ACC:  0.9785\n",
            "TEST| BATCH   49/ 157 | LOSS:  0.0627 | ACC:  0.9786\n",
            "TEST| BATCH   50/ 157 | LOSS:  0.0616 | ACC:  0.9791\n",
            "TEST| BATCH   51/ 157 | LOSS:  0.0607 | ACC:  0.9795\n",
            "TEST| BATCH   52/ 157 | LOSS:  0.0600 | ACC:  0.9796\n",
            "TEST| BATCH   53/ 157 | LOSS:  0.0591 | ACC:  0.9800\n",
            "TEST| BATCH   54/ 157 | LOSS:  0.0592 | ACC:  0.9800\n",
            "TEST| BATCH   55/ 157 | LOSS:  0.0592 | ACC:  0.9801\n",
            "TEST| BATCH   56/ 157 | LOSS:  0.0630 | ACC:  0.9799\n",
            "TEST| BATCH   57/ 157 | LOSS:  0.0623 | ACC:  0.9800\n",
            "TEST| BATCH   58/ 157 | LOSS:  0.0615 | ACC:  0.9803\n",
            "TEST| BATCH   59/ 157 | LOSS:  0.0618 | ACC:  0.9804\n",
            "TEST| BATCH   60/ 157 | LOSS:  0.0644 | ACC:  0.9797\n",
            "TEST| BATCH   61/ 157 | LOSS:  0.0648 | ACC:  0.9795\n",
            "TEST| BATCH   62/ 157 | LOSS:  0.0647 | ACC:  0.9793\n",
            "TEST| BATCH   63/ 157 | LOSS:  0.0642 | ACC:  0.9794\n",
            "TEST| BATCH   64/ 157 | LOSS:  0.0639 | ACC:  0.9795\n",
            "TEST| BATCH   65/ 157 | LOSS:  0.0630 | ACC:  0.9798\n",
            "TEST| BATCH   66/ 157 | LOSS:  0.0629 | ACC:  0.9794\n",
            "TEST| BATCH   67/ 157 | LOSS:  0.0646 | ACC:  0.9788\n",
            "TEST| BATCH   68/ 157 | LOSS:  0.0640 | ACC:  0.9791\n",
            "TEST| BATCH   69/ 157 | LOSS:  0.0638 | ACC:  0.9792\n",
            "TEST| BATCH   70/ 157 | LOSS:  0.0632 | ACC:  0.9795\n",
            "TEST| BATCH   71/ 157 | LOSS:  0.0636 | ACC:  0.9791\n",
            "TEST| BATCH   72/ 157 | LOSS:  0.0635 | ACC:  0.9789\n",
            "TEST| BATCH   73/ 157 | LOSS:  0.0632 | ACC:  0.9790\n",
            "TEST| BATCH   74/ 157 | LOSS:  0.0626 | ACC:  0.9793\n",
            "TEST| BATCH   75/ 157 | LOSS:  0.0621 | ACC:  0.9796\n",
            "TEST| BATCH   76/ 157 | LOSS:  0.0627 | ACC:  0.9792\n",
            "TEST| BATCH   77/ 157 | LOSS:  0.0633 | ACC:  0.9789\n",
            "TEST| BATCH   78/ 157 | LOSS:  0.0641 | ACC:  0.9790\n",
            "TEST| BATCH   79/ 157 | LOSS:  0.0632 | ACC:  0.9792\n",
            "TEST| BATCH   80/ 157 | LOSS:  0.0626 | ACC:  0.9795\n",
            "TEST| BATCH   81/ 157 | LOSS:  0.0619 | ACC:  0.9797\n",
            "TEST| BATCH   82/ 157 | LOSS:  0.0611 | ACC:  0.9800\n",
            "TEST| BATCH   83/ 157 | LOSS:  0.0604 | ACC:  0.9802\n",
            "TEST| BATCH   84/ 157 | LOSS:  0.0597 | ACC:  0.9805\n",
            "TEST| BATCH   85/ 157 | LOSS:  0.0590 | ACC:  0.9807\n",
            "TEST| BATCH   86/ 157 | LOSS:  0.0583 | ACC:  0.9809\n",
            "TEST| BATCH   87/ 157 | LOSS:  0.0577 | ACC:  0.9811\n",
            "TEST| BATCH   88/ 157 | LOSS:  0.0571 | ACC:  0.9814\n",
            "TEST| BATCH   89/ 157 | LOSS:  0.0565 | ACC:  0.9816\n",
            "TEST| BATCH   90/ 157 | LOSS:  0.0562 | ACC:  0.9816\n",
            "TEST| BATCH   91/ 157 | LOSS:  0.0556 | ACC:  0.9818\n",
            "TEST| BATCH   92/ 157 | LOSS:  0.0554 | ACC:  0.9818\n",
            "TEST| BATCH   93/ 157 | LOSS:  0.0556 | ACC:  0.9815\n",
            "TEST| BATCH   94/ 157 | LOSS:  0.0559 | ACC:  0.9814\n",
            "TEST| BATCH   95/ 157 | LOSS:  0.0556 | ACC:  0.9816\n",
            "TEST| BATCH   96/ 157 | LOSS:  0.0552 | ACC:  0.9816\n",
            "TEST| BATCH   97/ 157 | LOSS:  0.0549 | ACC:  0.9816\n",
            "TEST| BATCH   98/ 157 | LOSS:  0.0543 | ACC:  0.9818\n",
            "TEST| BATCH   99/ 157 | LOSS:  0.0538 | ACC:  0.9820\n",
            "TEST| BATCH  100/ 157 | LOSS:  0.0532 | ACC:  0.9822\n",
            "TEST| BATCH  101/ 157 | LOSS:  0.0528 | ACC:  0.9824\n",
            "TEST| BATCH  102/ 157 | LOSS:  0.0529 | ACC:  0.9824\n",
            "TEST| BATCH  103/ 157 | LOSS:  0.0541 | ACC:  0.9821\n",
            "TEST| BATCH  104/ 157 | LOSS:  0.0560 | ACC:  0.9818\n",
            "TEST| BATCH  105/ 157 | LOSS:  0.0555 | ACC:  0.9820\n",
            "TEST| BATCH  106/ 157 | LOSS:  0.0553 | ACC:  0.9819\n",
            "TEST| BATCH  107/ 157 | LOSS:  0.0549 | ACC:  0.9820\n",
            "TEST| BATCH  108/ 157 | LOSS:  0.0545 | ACC:  0.9822\n",
            "TEST| BATCH  109/ 157 | LOSS:  0.0540 | ACC:  0.9824\n",
            "TEST| BATCH  110/ 157 | LOSS:  0.0535 | ACC:  0.9825\n",
            "TEST| BATCH  111/ 157 | LOSS:  0.0532 | ACC:  0.9827\n",
            "TEST| BATCH  112/ 157 | LOSS:  0.0528 | ACC:  0.9827\n",
            "TEST| BATCH  113/ 157 | LOSS:  0.0526 | ACC:  0.9827\n",
            "TEST| BATCH  114/ 157 | LOSS:  0.0522 | ACC:  0.9829\n",
            "TEST| BATCH  115/ 157 | LOSS:  0.0517 | ACC:  0.9830\n",
            "TEST| BATCH  116/ 157 | LOSS:  0.0513 | ACC:  0.9832\n",
            "TEST| BATCH  117/ 157 | LOSS:  0.0512 | ACC:  0.9832\n",
            "TEST| BATCH  118/ 157 | LOSS:  0.0509 | ACC:  0.9832\n",
            "TEST| BATCH  119/ 157 | LOSS:  0.0505 | ACC:  0.9833\n",
            "TEST| BATCH  120/ 157 | LOSS:  0.0500 | ACC:  0.9835\n",
            "TEST| BATCH  121/ 157 | LOSS:  0.0496 | ACC:  0.9836\n",
            "TEST| BATCH  122/ 157 | LOSS:  0.0494 | ACC:  0.9836\n",
            "TEST| BATCH  123/ 157 | LOSS:  0.0490 | ACC:  0.9837\n",
            "TEST| BATCH  124/ 157 | LOSS:  0.0487 | ACC:  0.9839\n",
            "TEST| BATCH  125/ 157 | LOSS:  0.0483 | ACC:  0.9840\n",
            "TEST| BATCH  126/ 157 | LOSS:  0.0481 | ACC:  0.9840\n",
            "TEST| BATCH  127/ 157 | LOSS:  0.0482 | ACC:  0.9840\n",
            "TEST| BATCH  128/ 157 | LOSS:  0.0478 | ACC:  0.9841\n",
            "TEST| BATCH  129/ 157 | LOSS:  0.0475 | ACC:  0.9843\n",
            "TEST| BATCH  130/ 157 | LOSS:  0.0472 | ACC:  0.9844\n",
            "TEST| BATCH  131/ 157 | LOSS:  0.0469 | ACC:  0.9845\n",
            "TEST| BATCH  132/ 157 | LOSS:  0.0467 | ACC:  0.9846\n",
            "TEST| BATCH  133/ 157 | LOSS:  0.0463 | ACC:  0.9847\n",
            "TEST| BATCH  134/ 157 | LOSS:  0.0464 | ACC:  0.9847\n",
            "TEST| BATCH  135/ 157 | LOSS:  0.0460 | ACC:  0.9848\n",
            "TEST| BATCH  136/ 157 | LOSS:  0.0457 | ACC:  0.9849\n",
            "TEST| BATCH  137/ 157 | LOSS:  0.0454 | ACC:  0.9851\n",
            "TEST| BATCH  138/ 157 | LOSS:  0.0450 | ACC:  0.9852\n",
            "TEST| BATCH  139/ 157 | LOSS:  0.0447 | ACC:  0.9853\n",
            "TEST| BATCH  140/ 157 | LOSS:  0.0444 | ACC:  0.9854\n",
            "TEST| BATCH  141/ 157 | LOSS:  0.0447 | ACC:  0.9854\n",
            "TEST| BATCH  142/ 157 | LOSS:  0.0447 | ACC:  0.9853\n",
            "TEST| BATCH  143/ 157 | LOSS:  0.0444 | ACC:  0.9854\n",
            "TEST| BATCH  144/ 157 | LOSS:  0.0441 | ACC:  0.9855\n",
            "TEST| BATCH  145/ 157 | LOSS:  0.0438 | ACC:  0.9856\n",
            "TEST| BATCH  146/ 157 | LOSS:  0.0435 | ACC:  0.9857\n",
            "TEST| BATCH  147/ 157 | LOSS:  0.0432 | ACC:  0.9858\n",
            "TEST| BATCH  148/ 157 | LOSS:  0.0429 | ACC:  0.9859\n",
            "TEST| BATCH  149/ 157 | LOSS:  0.0427 | ACC:  0.9859\n",
            "TEST| BATCH  150/ 157 | LOSS:  0.0424 | ACC:  0.9860\n",
            "TEST| BATCH  151/ 157 | LOSS:  0.0433 | ACC:  0.9858\n",
            "TEST| BATCH  152/ 157 | LOSS:  0.0439 | ACC:  0.9857\n",
            "TEST| BATCH  153/ 157 | LOSS:  0.0445 | ACC:  0.9856\n",
            "TEST| BATCH  154/ 157 | LOSS:  0.0445 | ACC:  0.9856\n",
            "TEST| BATCH  155/ 157 | LOSS:  0.0446 | ACC:  0.9856\n",
            "TEST| BATCH  156/ 157 | LOSS:  0.0445 | ACC:  0.9856\n",
            "TEST| BATCH  157/ 157 | LOSS:  0.0442 | ACC:  0.9857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhkZRbWim1ck",
        "outputId": "f8d3bb8b-9eca-4f06-9a39-875fbf2f6c29"
      },
      "source": [
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (relu1): ReLU()\n",
              "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (drop2): Dropout2d(p=0.5, inplace=False)\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (relu2): ReLU()\n",
              "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
              "  (relu1_fc1): ReLU()\n",
              "  (drop1_fc1): Dropout2d(p=0.5, inplace=False)\n",
              "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7g0YrCyrjBuQ",
        "outputId": "e5b63f17-6692-47f1-d16e-ad3c5f5a28fd"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cpu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    }
  ]
}